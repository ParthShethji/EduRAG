{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"Loads a PDF and extracts text from it.\"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    return pages\n",
    "\n",
    "pdf_path = \"../ikigai.pdf\"\n",
    "documents = load_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "IKIGAI \n",
      "1. What is your reason for being? \n",
      "2. Whatever you do, don’t retire! \n",
      "3. Having a clearly defined ikigai brings satisfaction, \n",
      "happiness, and meaning to our lives. \n",
      "4. Their blood tests reveal fewer free radicals (which are \n",
      "responsible for cellular aging), as a result of drinking tea \n",
      "and eating until their stomachs are only 80 percent full \n",
      "5. the keys to longevity are diet, exercise, finding a purpose \n",
      "in life (an ikigai), and forming strong social ties—that is,\n",
      "\n",
      "Chunk 2:\n",
      "in life (an ikigai), and forming strong social ties—that is, \n",
      "having a broad circle of friends and good family relations. \n",
      "6. Members of these communities manage their time well in \n",
      "order to reduce \n",
      "7. stress, consume little meat or processed foods, and drink \n",
      "alcohol in moderation.1 They don’t do strenuous exercise, \n",
      "but they do move every day, taking walks and working in \n",
      "their vegetable gardens \n",
      "8. Fill your belly to 80 percent.” Ancient wisdom advises\n",
      "\n",
      "Chunk 3:\n",
      "their vegetable gardens \n",
      "8. Fill your belly to 80 percent.” Ancient wisdom advises \n",
      "against eating until we are full. “Hara hachi bu\". The \n",
      "lesson to learn from this saying is that we should stop \n",
      "eating when we are starting to feel full.\n",
      "\n",
      "Chunk 4:\n",
      "9. By presenting their meals on many small plates, the \n",
      "Japanese tend to eat less \n",
      "10.Having five plates in front of you makes it seem like you \n",
      "are going to eat a lot, but what happens most of the time \n",
      "is that you end up feeling slightly hungry.  \n",
      "11.Moai: Connected for life - It is customary in Okinawa to \n",
      "form close bonds within local communities. A moai is an \n",
      "informal group of people with common interests who look \n",
      "out for one another.In this way, being part of a moai\n",
      "\n",
      "Chunk 5:\n",
      "out for one another.In this way, being part of a moai \n",
      "helps maintain emotional and financial stability. If a \n",
      "member of a moai is in financial trouble, he or she can get \n",
      "an advance from the group’s savings. the feeling of \n",
      "belonging and support gives the individual a sense of \n",
      "security and helps increase life expectancy \n",
      "12.mens sana in corpore sano” (“a sound mind in a sound \n",
      "body”): It reminds us that both mind and body are \n",
      "important, and that the health of one is connected to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(documents, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"Splits extracted text into manageable chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "text_chunks = chunk_text(documents)\n",
    "\n",
    "for i, chunk in enumerate(text_chunks[:5]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "\n",
    "# Initialize Gemini Embeddings\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_embeddings(text_chunks):\n",
    "    \"\"\"Converts text chunks into Gemini embeddings.\"\"\"\n",
    "    texts = [chunk.page_content for chunk in text_chunks]\n",
    "    embeddings = embeddings_model.embed_documents(texts)\n",
    "    return np.array(embeddings).astype(\"float32\"), texts\n",
    "\n",
    "# Convert text chunks to embeddings\n",
    "embeddings, texts = generate_gemini_embeddings(text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"faiss.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save text chunks separately\n",
    "with open(\"stored_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(texts, f)  # `texts` contains the original text chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"faiss.index\")\n",
    "\n",
    "# Load stored text chunks (stored separately in a pickle file)\n",
    "with open(\"stored_chunks.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)  # This should contain the chunked text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'generate_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract topics from the stored chunks\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m important_topics \u001b[38;5;241m=\u001b[39m \u001b[43mextract_topics_from_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracted Topics:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, important_topics)\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mextract_topics_from_chunks\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract key topics from stored FAISS chunks.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m combined_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)  \u001b[38;5;66;03m# Merge chunks for topic extraction\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m(\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract the most important topics from the following study material:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcombined_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m topics \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Convert response to list\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.generativeai' has no attribute 'generate_content'"
     ]
    }
   ],
   "source": [
    "def extract_topics_from_chunks(chunks):\n",
    "    \"\"\"Extract key topics from stored FAISS chunks.\"\"\"\n",
    "    combined_text = \"\\n\".join(chunks)  # Merge chunks for topic extraction\n",
    "\n",
    "    response = genai.generate_content(\n",
    "        model=\"gemini-pro\",\n",
    "        prompt=f\"Extract the most important topics from the following study material:\\n\\n{combined_text}\"\n",
    "    )\n",
    "\n",
    "    topics = response.text.split(\"\\n\")  # Convert response to list\n",
    "    return topics\n",
    "\n",
    "# Extract topics from the stored chunks\n",
    "important_topics = extract_topics_from_chunks(texts)\n",
    "print(\"\\nExtracted Topics:\\n\", important_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
